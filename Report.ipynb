{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443bb75d-6bec-4dd7-a351-c1d4b9c12a9a",
   "metadata": {},
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af006cc-9b86-43e0-b8b9-b80129743589",
   "metadata": {},
   "source": [
    "## Basics of Mobile Robotics, MICRO-452"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d704bbd-049b-4300-b193-ad024750db29",
   "metadata": {},
   "source": [
    "### Mehdi Krichen, Francesco Nonis, Michael Richter, Vincent Philippoz, Nathan Decurnex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43dca7a-6a90-418b-bca9-321ef47e1c75",
   "metadata": {},
   "source": [
    "# 1) Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf72940-d7c0-43e3-8eb8-50a44ab729a3",
   "metadata": {},
   "source": [
    "Our environment in which our thymio will evolve is a rectangle space composed of two A0 sheet. The size was choosen so that the Thymio could perform efficient and visible local avoidance moves.\n",
    "The background is green to imitate grass. It has been set to a uniform dark green to not intefer with the thymio's ground sensors (to contrast with the yellow of the crossroads).\n",
    "The world is also compose of roads and crossroads. The roads will be seen as obstacles. Obviously, the robot can use the crossroads to pass roads. The design of these modules stay simple: rectangular shapes. This is crucial to simplify the vision part. The crossroads were initially yellow and gray like real ones. But we further choose to change them to uniform yellow. This simplified the ground sensors measurements to update the filter, but made their detection in the vision part more difficult. \n",
    "\n",
    "The size of the crossroads and road were chosen to approximately be squares of Thymio's dimensions. At the beginning, we wanted to have another Thymio following the white line in the middle of the road. But the main part of the project took us more time than planned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5600657-579e-4222-a8c8-baa13ab1efe0",
   "metadata": {},
   "source": [
    "![env](img.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b451f17-c044-48b1-9e12-fdf65fb29b55",
   "metadata": {},
   "source": [
    "# 2) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1436b3-c595-4822-bbce-da6076ab3eb7",
   "metadata": {},
   "source": [
    "## 2.1) Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756eedc-cb30-4ec3-b1cf-9d7e1f334084",
   "metadata": {},
   "source": [
    "The vision part of the project is essentially used at initialization. It will be used to build an initial obstacle map of the environment. The processing will also find the Thymio initial location and orientation, and the target to reach.\n",
    "\n",
    "At the start, a picture of the whole environment is taken. The first step is to trim the initial image to obtain a frame with only the environment. We first tried to found the 4 corners of the map in order to perform a reshaping to account for perspective. But the task of finding the corners was not so simple for several reasons. We tried to have one particular shape on each corner so that we could perform template matching with it. But it was hard to detect 4 time the same shape in our environmental conditions; even by filtering our image, other shapes in our environment had regularly more correlation with the template than the real one. We thus thought to change the shape for each corner, but we were limited in the choice of easy distinguishable colors and shape. Moreover, choosing a particular shape could make hard the template detection due to perspective. It why we wanted to have circles. A last factor that made hard edge detection via templatematching is that we do not know the exact size of the template on the image; template matching is very sensitive to the size of the object we are trying to match. In other word, to make work correctly template matching, we should know in advance the size in pixels of our template on the image.  \n",
    "\n",
    "Before trying some other fancy techniques, we wanted to have a backup. It was to simply mask the green background region of our environment via template matching. This allowed us to have a rather good cropping of the picture as long as it has been taken far away enough; to avoid having straight line deformations. The result was good enough to keep it. We could have tried line detection, but the roads or the floor pattern outside of our map would have also been detected. Sometime to stay simple is the good solution.  \n",
    "\n",
    "Once the image trimmed, Template matching is again used to detect all the objects on the map. The 4 patterns of turning roads (4 orientations) and the two orientation of straight lines were used to detect the roads. Even with such a simple design, the sensitivity and selectivity of template matching was quite poor. Luminosity is also a decisive factor when trying to detect objects. A lot of hyperparameter needed to be tuned in order to achieve a sufficient road detection. We could have gone for simple uniform gray roads: it would have simplified the template search (also faster computation) and improved the selectivity/sensitivity. But at the beginning, a second robot was planned to follow the road: the central white line would have been used to track the path. \n",
    "\n",
    "The crossroads were detected by the same way as the roads. As expected, the result is also highly depending on the tuning of hyperparameters and lighting. The target position was finally decided to be a simple circle. This allowed no orientation dependence: allowing one single template search.\n",
    "\n",
    "Finally, the Thymio robot is detected thanks to two very small circle placed strategically at the back and front of the robot. Once the circles detected by template matching, a simple calculation is done to obtain the robot hole's position and orientation. We also tried a data driven method: Convolutionnal Neural Network (CNN) and Support Vector Regression to extract the robot position from the image as input. The major issue was to collect data and label it. Simple trials with 25 data points yielded unsatisfying results and stopped our search in this direction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38993d5-fae1-4293-b70d-1f327c411604",
   "metadata": {},
   "source": [
    "### Illustrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb5d30-0ce4-49d5-bcd7-0c3d1236e609",
   "metadata": {},
   "source": [
    "In this section, we present sub-optimal results and particularities that can be observed from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2dcbc1-cec7-4bf1-80b6-baa137a9b326",
   "metadata": {},
   "source": [
    "![env](images/angle1.jpg)\n",
    "\n",
    "![env](images/angle12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04aa06-c2a9-401f-b1e2-70d0584b2ef7",
   "metadata": {},
   "source": [
    "Above are illustrations on how sensitive the template matching, for a the road angle, is. The results for the four different orientation yielded the same kind of results showed above. This may be due to deformation of roads on the bottom of the image. The template matches better straight roads than certain correct road on the bottom.\n",
    "\n",
    "This situation only happens with real big images. When we were testing on smaller scaled map or screenshots of the map on the computer, this did not happen. This shows how image processing becomes harder in real conditions. We think that the template matching could have been improved a little bit: by taking the template image with the same camera that takes the real pictures shown. Currently, the template are the computer image (numerical ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ec289-e41f-4a50-b3f8-cdc9e1e890e0",
   "metadata": {},
   "source": [
    "This problem was essentially solved by using the high sensitivity of the template matching of the straight roads. This in some way is a bit cheating, but as long as it works...\n",
    "It was even our final way to find all the roads because it was the more robust we found. In fact the angle roads detection is useless at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051863f0-b1b5-4f84-84c1-309a3ff65190",
   "metadata": {},
   "source": [
    "![env](images/rd2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2bb29-8e49-43c8-a2ac-46598e11744c",
   "metadata": {},
   "source": [
    "![env](images/road44.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3381a8-a137-4ef1-af5f-bc1cbc0c8af5",
   "metadata": {},
   "source": [
    "We can see at which point, even the wrong orientation of the road can be detected easily. Some crossroads are also captured in the process. Even some opposite orientation roads are detected while the correct orientation is not (see second illustration on the bottom, the template orientation can be deduced: horizontal roads). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382259dd-a05b-4d06-b964-027ace759afb",
   "metadata": {},
   "source": [
    "![env](images/psg2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffff3ee-7f3d-47af-9027-1f3f84fe94fb",
   "metadata": {},
   "source": [
    "Note the crossroad design was change finally.\n",
    "Of course the same effect appears also for crossroads. The right crossroad is much more detected than the left one. If we want absolutely to detect the two horizontal crossroad, we need to decrease the threshold. This has a direct effect on the right crossroad: it appears larger; even some portion of the right crossroad combined to the road is more detectable than the left crossroad... \n",
    "\n",
    "The lighting is even more affecting the results than previously. We think it is because when the yellow is well lighted, it is almost seen like white from the camera. The figure shows a little bit this effect for the crossroad on the left of the image that appears brighter than the other ones.\n",
    "\n",
    "We finally decided to change the design of the crossroad to simplify the filter localization; uniform yellow. But it made their detection much harder. Finally, as the crossroads had no more white line in the middle, the road template matching could not anymore find them. So we could use only the straight lines pattern to extract all the road and by extent the crossroads; where the roads are not detected it is seen like a hole where the robot is allowed to pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb5327e-79e4-4512-a1d2-4e8813704895",
   "metadata": {},
   "source": [
    "### Real Code testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5d874-b9ff-4d81-a844-f9ad5c532b1b",
   "metadata": {},
   "source": [
    "If you want a real test implementation of the Image Processing part, you can run the cell below. It will print the current progression of the process and show several plots showing each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618581f-bb68-40c2-88f1-cbc41d584e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run IP/IP_methods.ipynb \n",
    "(x,y), orientation = pipeline_IP(plot=True)\n",
    "print(f'position of thymio is ({x},{y}), the x position is on the horizontal axis and y on the vertical axis. The zero is on the bottom left')\n",
    "print(f'its orientation is {orientation} [rad] (defined as [0,2Pi] with respect to the horizontal axis)') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89a6ec-5dd7-4679-8f00-d964ff55012e",
   "metadata": {},
   "source": [
    "Globally, image processing is very hard. Robust method to lightning, object orientation, deformation or camera type etc.. do not really exist. All lot of testing needed to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e4bc3-18a6-435c-a197-7e5b0cadd268",
   "metadata": {},
   "source": [
    "## 2.2) Path Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e4a27-41ac-41a0-b4c6-b8b18e896d95",
   "metadata": {},
   "source": [
    "Many algorithms exist for path planning. The one used in this project is the vision graph method. It has many disadvantages when building the map, but once it is built, the computation is very quick. Other algorithms were explored, but in an effort to shorten the report, they are explored in the _PathPlanning.ipynb_ notebook.\n",
    "\n",
    "The first step is to define a coordinate system. We choose an arbitrary corner as the origin and choose the max axis coordinates to be the physical dimensions of our map in mm:\n",
    "> 2*A0 : 1682 x 1188 mm\n",
    "\n",
    "Once chosen, we can read the CSV file of the obstacles produced by the vision module of the project. First, the image is read, then saved for ease of use and easy access. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07253ee3-3d97-4dce-8a09-a8122aa99d84",
   "metadata": {},
   "source": [
    "## 2.3) Global Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31615c66-79c2-422b-bb77-37ab10cfe840",
   "metadata": {},
   "source": [
    "## 2.4) Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da020e2-887e-4313-b68c-8479b2c49768",
   "metadata": {},
   "source": [
    "## 2.5) Localization\n",
    "### How the localization works\n",
    "#### Introduction\n",
    "The localization module objective is to estimate the Thymio's pose during the execution of the other modules. To achieve this goal, the module implements a particle filter which uses the motors speeds and the the ground proximity sensors. It proceeds in two steps : Odometry and Measurement.\n",
    "\n",
    "#### Odometry \n",
    "The odometry does the apriori estimation of the position by using the motors speeds and a model of the Thymio. The model is taken from the book *Elements of Robotics* by *Mordechai Ben-Ari and Francesco Mondada*, chapter 5.6.\n",
    "\n",
    "<img src='images_for_report/odometry_from_book.png' width=\"400\">\n",
    "\n",
    "It is assumed that we know $b$ the wheelbase, $d_l$ and $d_r$ the distance traveled by the left and right wheels respectively, $(x,y,\\phi)$ the initial pose. We want to compute the new pose $(x',y',\\phi')$. \n",
    "\n",
    "First, we compute $\\theta$, the angle of the arc followed by the robot. As the displacement is considered small, we can approximate $sin(x) \\approx x$. This means that we get $\\theta = \\frac{d_r-d_l}{b}$.\n",
    "\n",
    "Now we can find the vertical and horizontal displacements with these equations:\n",
    "\n",
    "$dx = \\frac{dl+dr}{2} cos(\\phi_0 + \\frac{\\theta}{2})$ ; $dy = \\frac{dl+dr}{2} sin(\\phi_0 + \\frac{\\theta}{2})$\n",
    "\n",
    "Finally, the new pose is given by:\n",
    "\n",
    "$(x_1,y_1,\\phi_1) = (x_0 + dx, y_0 +dy, \\phi_0+\\theta)$\n",
    "\n",
    "In the context of the map, the robot position is defined as follow:\n",
    "\n",
    "<img src='images_for_report/thymio_top_schema.png' width=\"150\">\n",
    "\n",
    "This image represents the Thymio seen from above. The dot represents the origin of the Thymio, the two grey rectangles are the ground sensors and the black rectangles are the wheels. The robot evolves in the same system of coordinates as the map.\n",
    "\n",
    "#### Measurement\n",
    "To add a feedback on the apriori odometry, the ground sensors measure the color of the map below the Thymio. This information is compared to the expected color given by the map.\n",
    "\n",
    "#### Why a particle filter ?\n",
    "At first, we wanted to implement a Kalman filter. However, as the Thymio's dynamics are non-linear, this solution was not feasible. The particle filter was chosen because of its robustness, execution speed and relative ease of implementation. Also, we thought it was interesting to implement this type of filter, as it was not done during the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a754b09",
   "metadata": {},
   "source": [
    "### Particle filter\n",
    "*A tutorial on how to implement a particle filter is presented [here](https://salzis.wordpress.com/2015/05/25/particle-filters-with-python/). Also, [this YouTube video](https://www.youtube.com/watch?v=aUkBa1zMKv4&ab_channel=AndreasSvensson) explains the basic principle of a particle filter.*\n",
    "\n",
    "The particle filter pseudocode is presented below:\n",
    "```python\n",
    "def particle_filter_one_step(particles, meas, Pt, Ps):\n",
    "    '''\n",
    "    Short: returns a set of particles for the next time step\n",
    "    \n",
    "    Inputs: \n",
    "    - particles: a vector of particles positions\n",
    "    - meas: the new incoming measurement\n",
    "    - Pt: a transition model Pt(new_position | old_position) (probability density function)\n",
    "    - Ps: a sensor model Ps(measurement | position) (probability density function)\n",
    "\n",
    "    Local variables: \n",
    "    - W: a vector of weights of the same size as Particles\n",
    "    '''\n",
    "    N = len(Particles) # number of particles\n",
    "    \n",
    "    for i in range(N):\n",
    "        # step 1: Transition for each particle\n",
    "        new_position = sample form Pt(new_position | particles[i])\n",
    "        particles[i] = new_position\n",
    "\n",
    "        # step 2: Assign weight to particles given the measurement\n",
    "        W[i] = Ps(meas | particles[i])\n",
    "\n",
    "        # step 3: Resample the particles\n",
    "        particles = resample(N, particles, W)\n",
    "    return [particles, W]\n",
    "\n",
    "particles = initial position of the robot\n",
    "while robot is moving:\n",
    "    old_robot_position = robot.position\n",
    "    # robot moves\n",
    "    meas = robot.measure()\n",
    "    Pt = robot.Pt\n",
    "    Ps = robot.Ps\n",
    "    [particles, Weights] = particle_filter_one_step(particles, meas, Pt, Ps)\n",
    "    robot.position = weighted_mean(particles, Weights)\n",
    "    robot.direction = weighted_vectorial_sum(particles)\n",
    "```\n",
    "\n",
    "The **transition** step is implemented using the ```_apriori_odometry``` function from the ```Moving_point``` class. To simulate the input noise (motor noise), the speed considered for the odometry is a random value generated by a gaussian. Its mean is the measured motor speed, and its standard deviation represents the measured speed noise (which is a hyperparameter).\n",
    "\n",
    "Each particles is assigned a **weight** which represents how likely it is that the particle is at the same spot than the real robot. This weight is the probability of the real measurement being generated by the particle. For example, if the ground sensor measures \"320\" and the expected measure at the particle location is \"120\", then the weight would be very small as it is very unlikely that the particle is at the same spot as the robot.\n",
    "\n",
    "A key part of the code is the **resampling**. step. In our case, it is simply implemented with a weighted random selection. As it makes no sense to resample particles of similar likelihood, the resampling is done only when the particles have diverse weights.\n",
    "\n",
    "\n",
    "\n",
    "Finally, once the particle filter step is done, the particles data is used to **estimate the robot pose**. The estimated position is the weighted average of the particles, and the estimated orientation is the vectorial weighted sum of all the particles directions.\n",
    "To implement the particle filter in a clean way, a system of classes is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c83ac9",
   "metadata": {},
   "source": [
    "### The system of classes\n",
    "The localization module consists of 3 classes:\n",
    "* ```Moving_point```: This is the base class. It defines the main functions.\n",
    "* ```Particle```: This class inherits the properties of ```Moving_point``` and adds some methods adapted to particles.\n",
    "* ```Pose```: This class inherits the properties of ```Moving_point``` and adds some methods adapted to a global point of view. This is the class used for interfacing with the other modules.\n",
    "\n",
    "#### Moving_point\n",
    "The ```Moving_point``` class entails the properties of an ideal Thymio robot. It can memorises its pose, do apriori odometry and check if the ground proximity sensors are out of the map. Also, getter and setter functions are provided to interface it if necessary.\n",
    "\n",
    "#### Particle\n",
    "The ```Particle``` class is used to simulate a particle of the particle filter. As a particle is simply an ideal Thymio, the ```Particle``` class inherits all the attributes of the ```Moving_point``` class. Furthermore, the particle is given a weight and some methods to compute it. The apriori odometry is also a bit modified to simulate the motor noise. Finally, a plotting function is added.\n",
    "\n",
    "#### Pose\n",
    "The ```Pose``` class is in charge of running the particle filter and processing the results. To do so, it is itself a ```Moving_point``` with some extra attributes and methods. These new properties allow the ```Pose``` class to simulate the particles, compute their weights, resample and estimate the position of the real Thymio. All of this is done with one call of the function ```update```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ddff02",
   "metadata": {},
   "source": [
    "### Hyperparameters : How to make the code work on my Thymio ?\n",
    "Every Thymio has unique characteristics. Therefore, one must calibrate some hyperparameters to expect good performances of the odometry. The hyperparameters to tune are :\n",
    "* ```SPEED_STD```: The standard deviation of each motor\n",
    "* ```PROX_L_STD```: The standard deviation of the left ground sensor\n",
    "* ```PROX_R_STD```: The standard deviation of the right ground sensor\n",
    "* ```FEATURE2PROX_L```: The mean value read by the left ground sensor for each color\n",
    "* ```FEATURE2PROX_R```: The mean value read by the right ground sensor for each color\n",
    "* ```SPEED_CONVERSION_FACTOR```: The conversion factor to go from Thymio units to mm/s.\n",
    "\n",
    "To tune all these values, the next procedures have to be followed.\n",
    "\n",
    "#### Procedure 1: Movements 1\n",
    "To make the movement calibration, you can run the code below. It will set ```SPEED_STD```, ```PROX_L_STD``` and ```PROX_R_STD```.\n",
    "\n",
    "```python\n",
    "speeds_l = []\n",
    "speeds_r = []\n",
    "proxs_l = []\n",
    "proxs_r = []\n",
    "\n",
    "for s in range(len(speed_to_test)):\n",
    "    # Start the motors\n",
    "    motor_left_target = speed\n",
    "    motor_right_target = speed\n",
    "    \n",
    "    # Measure the speeds\n",
    "    for i in range(nb_calibration_samples):\n",
    "        speeds_l.append(motor_left_speed)                \n",
    "        speeds_r.append(motor_right_speed)\n",
    "        proxs_l.append(prox_ground_delta[0])\n",
    "        proxs_r.append(prox_ground_delta[1])\n",
    "        sleep(period)\n",
    "    \n",
    "    # Stop the motors\n",
    "    motor_left_target = 0\n",
    "    motor_right_target = 0\n",
    "    \n",
    "# Compute the standard deviations\n",
    "SPEED_STD[0][str(s)] = np.std(speeds_l)\n",
    "SPEED_STD[1][str(s)] = np.std(speeds_r)\n",
    "PROX_L_STD = np.std(proxs_l)\n",
    "PROX_R_STD = np.std(proxs_r)\n",
    "```\n",
    "\n",
    "#### Procedure 2: Colors\n",
    "To make the color calibration, you can run the code below. It will set ```FEATURE2PROX_L``` and ```FEATURE2PROX_L```.\n",
    "```python\n",
    "for color in colors_to_test:\n",
    "    proxs_l = []\n",
    "    proxs_r = []\n",
    "\n",
    "    # Measure the color\n",
    "    print(f\"Place the robot on '{color}' in the next 5 seconds.\")\n",
    "    sleep(5) # To have time to move the robot\n",
    "    for i in range(nb_calibration_samples):\n",
    "        proxs_l.append(prox_ground_delta[0])\n",
    "        proxs_r.append(prox_ground_delta[1])\n",
    "        sleep(period)\n",
    "\n",
    "    # Compute the mean measurement\n",
    "    FEATURE2PROX_L[color] = np.mean(proxs_l)\n",
    "    FEATURE2PROX_R[color] = np.mean(proxs_r)\n",
    "```\n",
    "\n",
    "#### Procedure 3: Movements 2\n",
    "Only one hyperparameters has to be tuned by hand : ```SPEED_CONVERSION_FACTOR```. To do so, place the Thymio on the map, with a straight line access to its objective. Run the global code until the Thymio stops. If it overshot the goal, increase the  ```SPEED_CONVERSION_FACTOR```. If it undershot, do the opposite. Repeat the process until the results are satisfying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007114a",
   "metadata": {},
   "source": [
    "### Interfacing : How to use the localisation module ?\n",
    "To run the odometry, one must first create an instance of the class Pose:\n",
    "```python\n",
    "myPose = Pose(x=x0, y=y0, theta=theta0)\n",
    "```\n",
    "You can also set ```isLost``` and ```nb_particles```, but they are not mandatory.\n",
    "\n",
    "\n",
    "Then, in your code where you make the thymio move, you call the method ```update```:\n",
    "\n",
    "```python\n",
    "myPose.update(speed, prox, dt, myMap)\n",
    "```\n",
    "This functions does everything for you, it updates the inner variables automatically. You can see in details what are the arguments by using ``` help(Pose.update)```. You can also do this for any function of any class.\n",
    "\n",
    "If you want to know where the robot is, call the method ```myPose.get_coords()```. It returns the pose as a dict.\n",
    "\n",
    "You can also set the pose using ```myPose.set_coords(args)```. In ```args```, you can either put ```x, y, theta``` or a dict ```{'x':x, 'y':y, 'theta':theta}```. This way, this code works :\n",
    "```python\n",
    "myPose.set_coords(x0, y0, theta0)\n",
    "# do something with myPose\n",
    "myCoords = myPose.get_coords()\n",
    "# do something with myCoords\n",
    "myPose.set_coords(myCoords)\n",
    "```\n",
    "\n",
    "The pose can be plotted using ```myPose.plot()```. Optional boolean arguments include ```plot_particles``` and ```plot_theta```.\n",
    "\n",
    "\n",
    "In the cell below, you can test a working example of simulation. It assigns randomly the speeds of the motors and plots the pose every 5 iterations.\n",
    "\n",
    "For it to work, one must open a valid map called myMap in the same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a56e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SPEED = 560\n",
    "MIN_SPEED = 100\n",
    "MAX_PROX = 1000\n",
    "DT = 0.2\n",
    "NB_PARTICLES = 100\n",
    "\n",
    "\n",
    "for i in range(40):\n",
    "    speed = [random.randrange(MIN_SPEED, MAX_SPEED), random.randrange(MIN_SPEED, MAX_SPEED)]\n",
    "    prox = myPose._expected_prox(myMap)\n",
    "    \n",
    "    # add noise on the prox\n",
    "    prox[0] += random.gauss(0, PROX_L_STD)\n",
    "    prox[1] += random.gauss(0, PROX_R_STD)\n",
    "\n",
    "    myPose.update(speed, prox, DT, myMap)\n",
    "    \n",
    "    # plot only 1 on 5 steps\n",
    "    if i%5 == 0:\n",
    "        myPose.plot(plot_theta=False, theta_size=5, plot_particles=True)\n",
    "    \n",
    "plt.scatter(0, 0, marker='+')\n",
    "plt.xlim(0, MAP_REAL[0])\n",
    "plt.ylim(0, MAP_REAL[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Pose.plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b6d69-1813-4ec5-bf28-5c8fd7923e5f",
   "metadata": {},
   "source": [
    "# 3) Conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f38d6-5364-4ce1-992d-109fa1af8cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
