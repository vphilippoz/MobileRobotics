{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443bb75d-6bec-4dd7-a351-c1d4b9c12a9a",
   "metadata": {},
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af006cc-9b86-43e0-b8b9-b80129743589",
   "metadata": {},
   "source": [
    "## Basics of Mobile Robotics, MICRO-452"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d704bbd-049b-4300-b193-ad024750db29",
   "metadata": {},
   "source": [
    "### Mehdi Krichen, Francesco Nonis, Michael Richter, Vincent Philippoz, Nathan Decurnex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43dca7a-6a90-418b-bca9-321ef47e1c75",
   "metadata": {},
   "source": [
    "# 1) Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf72940-d7c0-43e3-8eb8-50a44ab729a3",
   "metadata": {},
   "source": [
    "Our environment in which our thymio will evolve is a rectangle space composed of two A0 sheet. The background is green to imitate grass. It has been set to a uniform dark green to not intefer with the thymio's ground sensors.\n",
    "The world is also compose of roads and crossroads. The roads will be seen as obstacles. Obviously, the robot can use the crossroads to pass roads. The design of these modules stay simple: rectangular shapes. This is crucial to simplify the vision part. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5600657-579e-4222-a8c8-baa13ab1efe0",
   "metadata": {},
   "source": [
    "![env](images/env.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b451f17-c044-48b1-9e12-fdf65fb29b55",
   "metadata": {},
   "source": [
    "# 2) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1436b3-c595-4822-bbce-da6076ab3eb7",
   "metadata": {},
   "source": [
    "## 2.1) Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756eedc-cb30-4ec3-b1cf-9d7e1f334084",
   "metadata": {},
   "source": [
    "The vision part of the project is essentially used at initialization. It will be used to build an initial obstacle map of the environment. The processing will also find the Thymio initial location and orientation, and the target to reach.\n",
    "\n",
    "At the start, a picture of the whole environment is taken. The first step is to trim the initial image to obtain a frame with only the environment. We first tried to found the 4 corners of the map in order to perform a reshaping to account for perspective. But the task of finding the corners was not so simple for several reasons. We tried to have one particular shape on each corner so that we could perform template matching with it. But it was hard to detect 4 time the same shape in our environmental conditions; even by filtering our image, other shapes in our environment had regularly more correlation with the template than the real one. We thus thought to change the shape for each corner, but we were limited in the choice of easy distinguishable colors and shape. Moreover, choosing a particular shape could make hard the template detection due to perspective. It why we wanted to have circles. A last factor that made hard edge detection via templatematching is that we do not know the exact size of the template on the image; template matching is very sensitive to the size of the object we are trying to match. In other word, to make work correctly template matching, we should know in advance the size in pixels of our template on the image.  \n",
    "\n",
    "Before trying some other fancy techniques, we wanted to have a backup. It was to simply mask the green background region of our environment via template matching. This allowed us to have a rather good cropping of the picture as long as it has been taken far away enough; to avoid having straight line deformations. The result was good enough to keep it. We could have tried line detection, but the roads or the floor pattern outside of our map would have also been detected. Sometime to stay simple is the good solution.  \n",
    "\n",
    "Once the image trimmed, Template matching is again used to detect all the objects on the map. The 4 patterns of turning roads (4 orientations) and the two orientation of straight lines were used to detect the roads. Even with such a simple design, the sensitivity and selectivity of template matching was quite poor. Luminosity is also a decisive factor when trying to detect objects. A lot of hyperparameter needed to be tuned in order to achieve a sufficient road detection. We could have gone for simple uniform gray roads: it would have simplified the template search (also faster computation) and improved the selectivity/sensitivity. But at the beginning, a second robot was planned to follow the road: the central white line would have been used to track the path. \n",
    "\n",
    "The crossroads were detected by the same way as the roads. As expected, the result is also highly depending on the tuning of hyperparameters and lighting. The target position was finally decided to be a simple circle. This allowed no orientation dependence: allowing one single template search.\n",
    "\n",
    "Finally, the Thymio robot is detected thanks to two very small circle placed strategically at the back and front of the robot. Once the circles detected by template matching, a simple calculation is done to obtain the robot hole's position and orientation. We also tried a data driven method: Convolutionnal Neural Network (CNN) and Support Vector Regression to extract the robot position from the image as input. The major issue was to collect data and label it. Simple trials with 25 data points yielded unsatisfying results and stopped our search in this direction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38993d5-fae1-4293-b70d-1f327c411604",
   "metadata": {},
   "source": [
    "### Illustrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb5d30-0ce4-49d5-bcd7-0c3d1236e609",
   "metadata": {},
   "source": [
    "In this section, we present sub-optimal results and particularities that can be observed from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2dcbc1-cec7-4bf1-80b6-baa137a9b326",
   "metadata": {},
   "source": [
    "![env](images/angle1.jpg)\n",
    "\n",
    "![env](images/angle12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04aa06-c2a9-401f-b1e2-70d0584b2ef7",
   "metadata": {},
   "source": [
    "Above are illustrations on how sensitive the template matching, for a the road angle, is. The results for the four different orientation yielded the same kind of results showed above. This may be due to deformation of roads on the bottom of the image. The template matches better straight roads than certain correct road on the bottom.\n",
    "\n",
    "This situation only happens with real big images. When we were testing on smaller scaled map or screenshots of the map on the computer, this did not happen. This shows how image processing becomes harder in real conditions. We think that the template matching could have been improved a little bit: by taking the template image with the same camera that takes the real pictures shown. Currently, the template are the computer image (numerical ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ec289-e41f-4a50-b3f8-cdc9e1e890e0",
   "metadata": {},
   "source": [
    "This problem was essentially solved by using the high sensitivity of the template matching of the straight roads. This in some way is a bit cheating, but as long as it works..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051863f0-b1b5-4f84-84c1-309a3ff65190",
   "metadata": {},
   "source": [
    "![env](images/rd2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2bb29-8e49-43c8-a2ac-46598e11744c",
   "metadata": {},
   "source": [
    "![env](images/road44.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3381a8-a137-4ef1-af5f-bc1cbc0c8af5",
   "metadata": {},
   "source": [
    "We can see at which point, even the wrong orientation of the road can be detected easily. Some crossroads are also captured in the process. Even some opposite orientation roads are detected while the correct orientation is not (see second illustration on the bottom, the template orientation can be deduced: horizontal roads). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382259dd-a05b-4d06-b964-027ace759afb",
   "metadata": {},
   "source": [
    "![env](images/psg2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffff3ee-7f3d-47af-9027-1f3f84fe94fb",
   "metadata": {},
   "source": [
    "Of course the same effect appears also for crossroads. The right crossroad is much more detected than the left one. If we want absolutely to detect the two horizontal crossroad, we need to decrease the threshold. This has a direct effect on the right crossroad: it appears larger; even some portion of the right crossroad combined to the road is more detectable than the left crossroad... \n",
    "\n",
    "The lighting is even more affecting the results than previously. We think it is because when the yellow is well lighted, it is almost seen like white from the camera. The figure shows a little bit this effect for the crossroad on the left of the image that appears brighter than the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb5327e-79e4-4512-a1d2-4e8813704895",
   "metadata": {},
   "source": [
    "### Real testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5d874-b9ff-4d81-a844-f9ad5c532b1b",
   "metadata": {},
   "source": [
    "If you want a real test implementation of the Image Processing part, you can run the cell below. It will print the current progression of the process and show several plots showing each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618581f-bb68-40c2-88f1-cbc41d584e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run IP/IP_methods.ipynb \n",
    "(x,y), orientation = pipeline_IP(plot=True)\n",
    "print(f'position of thymio is ({x},{y}), the x position is on the horizontal axis and y on the vertical axis. The zero is on the bottom left')\n",
    "print(f'its orientation is {orientation} [rad] (defined as [0,2Pi] with respect to the horizontal axis)') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89a6ec-5dd7-4679-8f00-d964ff55012e",
   "metadata": {},
   "source": [
    "Globally, image processing is very hard. Robust method to lightning, object orientation, deformation or camera type etc.. do not really exist. All lot of testing needed to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e4bc3-18a6-435c-a197-7e5b0cadd268",
   "metadata": {},
   "source": [
    "## 2.2) Path Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07253ee3-3d97-4dce-8a09-a8122aa99d84",
   "metadata": {},
   "source": [
    "## 2.3) Global Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31615c66-79c2-422b-bb77-37ab10cfe840",
   "metadata": {},
   "source": [
    "## 2.4) Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da020e2-887e-4313-b68c-8479b2c49768",
   "metadata": {},
   "source": [
    "## 2.5) Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b6d69-1813-4ec5-bf28-5c8fd7923e5f",
   "metadata": {},
   "source": [
    "# 3) Conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f38d6-5364-4ce1-992d-109fa1af8cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
