{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443bb75d-6bec-4dd7-a351-c1d4b9c12a9a",
   "metadata": {},
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af006cc-9b86-43e0-b8b9-b80129743589",
   "metadata": {},
   "source": [
    "## Basics of Mobile Robotics, MICRO-452"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d704bbd-049b-4300-b193-ad024750db29",
   "metadata": {},
   "source": [
    "### Mehdi Krichen, Francesco Nonis, Michael Richter, Vincent Philippoz, Nathan Decurnex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43dca7a-6a90-418b-bca9-321ef47e1c75",
   "metadata": {},
   "source": [
    "# 1) Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf72940-d7c0-43e3-8eb8-50a44ab729a3",
   "metadata": {},
   "source": [
    "Our environment in which our thymio will evolve is a rectangle space composed of two A0 sheet. The background is green to imitate grass. It has been set to a uniform dark green to not intefer with the thymio's ground sensors.\n",
    "The world is also compose of roads and crossroads. The roads will be seen as obstacles. Obviously, the robot can use the crossroads to pass roads. The design of these modules stay simple: rectangular shapes. This is crucial to simplify the vision part. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5600657-579e-4222-a8c8-baa13ab1efe0",
   "metadata": {},
   "source": [
    "![env](images/env.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b451f17-c044-48b1-9e12-fdf65fb29b55",
   "metadata": {},
   "source": [
    "# 2) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1436b3-c595-4822-bbce-da6076ab3eb7",
   "metadata": {},
   "source": [
    "## 2.1) Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756eedc-cb30-4ec3-b1cf-9d7e1f334084",
   "metadata": {},
   "source": [
    "The vision part of the project is essentially used at initialization. It will be used to build an initial obstacle map of the environment. The processing will also find the Thymio initial location and orientation, and the target to reach.\n",
    "\n",
    "At the start, a picture of the whole environment is taken. The first step is to trim the initial image to obtain a frame with only the environment. We first tried to found the 4 corners of the map in order to perform a reshaping to account for perspective. But the task of finding the corners was not so simple for several reasons. We tried to have one particular shape on each corner so that we could perform template matching with it. But it was hard to detect 4 time the same shape in our environmental conditions; even by filtering our image, other shapes in our environment had regularly more correlation with the template than the real one. We thus thought to change the shape for each corner, but we were limited in the choice of easy distinguishable colors and shape. Moreover, choosing a particular shape could make hard the template detection due to perspective. It why we wanted to have circles. A last factor that made hard edge detection via templatematching is that we do not know the exact size of the template on the image; template matching is very sensitive to the size of the object we are trying to match. In other word, to make work correctly template matching, we should know in advance the size in pixels of our template on the image.  \n",
    "\n",
    "Before trying some other fancy techniques, we wanted to have a backup. It was to simply mask the green background region of our environment via template matching. This allowed us to have a rather good cropping of the picture as long as it has been taken far away enough; to avoid having straight line deformations. The result was good enough to keep it. We could have tried line detection, but the roads or the floor pattern outside of our map would have also been detected. Sometime to stay simple is the good solution.  \n",
    "\n",
    "Once the image trimmed, Template matching is again used to detect all the objects on the map. The 4 patterns of turning roads (4 orientations) and the two orientation of straight lines were used to detect the roads. Even with such a simple design, the sensitivity and selectivity of template matching was quite poor. Luminosity is also a decisive factor when trying to detect objects. A lot of hyperparameter needed to be tuned in order to achieve a sufficient road detection. We could have gone for simple uniform gray roads: it would have simplified the template search (also faster computation) and improved the selectivity/sensitivity. But at the beginning, a second robot was planned to follow the road: the central white line would have been used to track the path. \n",
    "\n",
    "The crossroads were detected by the same way as the roads. As expected, the result is also highly depending on the tuning of hyperparameters and lighting. The target position was finally decided to be a simple circle. This allowed no orientation dependence: allowing one single template search.\n",
    "\n",
    "Finally, the Thymio robot is detected thanks to two very small circle placed strategically at the back and front of the robot. Once the circles detected by template matching, a simple calculation is done to obtain the robot hole's position and orientation. We also tried a data driven method: Convolutionnal Neural Network (CNN) and Support Vector Regression to extract the robot position from the image as input. The major issue was to collect data and label it. Simple trials with 25 data points yielded unsatisfying results and stopped our search in this direction. \n",
    "\n",
    "Globally, image processing is very hard. Robust method to lightning, object orientation or deformation etc.. do not really exist. All lot of testing needed to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321451d0-f038-4ab9-85ca-2deffa22bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### add illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b6d69-1813-4ec5-bf28-5c8fd7923e5f",
   "metadata": {},
   "source": [
    "# 3) Conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f38d6-5364-4ce1-992d-109fa1af8cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
